---
title: "UMD_infochallenge"
output:
  html_document:
    df_print: paged
---
##Data
#Input data and library
```{r setup, include=FALSE}
library(readxl)
library(corrgram)
library(bestglm)
library(readr)
library(dplyr)
library(pROC)
library(fastDummies)
ppp_removed_ga <- read_excel("C:/Users/dtl62/Downloads/Data Analytics L3_ Paycheck Protection Program - UMD Journalism/Data_PPP/ppp-removed-ga.xlsx", 
     col_types = c("text", "numeric", "text", 
         "text", "text", "text", "text", "text", 
         "numeric", "date", "text", "text", 
         "text", "text", "text", "text", "numeric", 
         "numeric", "numeric", "numeric", 
         "numeric", "text", "text", "text", 
         "text", "text", "text", "text", "text", 
         "text", "text", "text", "text", "text", 
         "text", "text", "text", "numeric", 
         "text", "text", "text"))
ppp_removed_ga$date_approved=as.character(ppp_removed_ga$date_approved)
ppp_applicants_ga_full <- read_csv("C:/Users/dtl62/Downloads/Data Analytics L3_ Paycheck Protection Program - UMD Journalism/Data_PPP/ppp_applicants_ga_full.csv", 
     col_types = cols(date_approved = col_character(), 
         loan_number = col_character(), sba_office_code = col_character(), 
         loan_status_date = col_character(), 
         originating_lender_location_id = col_character(), 
         forgiveness_date = col_character()))
ppp_removed_ga=data.frame(ppp_removed_ga)
ppp_applicants_ga_full=data.frame(ppp_applicants_ga_full)
```

#Merge and process data(date,created removed col)
```{r}
#summary(ppp_removed_ga)
#summary(ppp_applicants_ga_full)
#typeof(ppp_applicants_ga_full)
#typeof(ppp_removed_ga)
#str(ppp_removed_ga)
#str(ppp_applicants_ga_full)
ppp_removed_ga$removed='removed'
ppp_applicants_ga_full$removed='not removed'

ppp_full=rbind(ppp_applicants_ga_full,ppp_removed_ga)
ppp_full$date_approved=as.Date(paste(ppp_full$date_approved))
#typeof(ppp_full$forgiveness_date)
ppp_full$forgiveness_date=as.Date(paste(ppp_full$forgiveness_date),'%Y-%m-%d')
ppp_full$loan_status_date=as.Date(paste(ppp_full$loan_status_date),'%Y-%m-%d')
ppp_full$removed=as.factor(ppp_full$removed)
ppp_applicants_ga_full
ppp_removed_ga
set.seed(12345)
ppp_test <- sample_n(ppp_full, 10000)

#ppp_test=head(ppp_full,100)
```

```{r}
corrgram(ppp_full,main="PPP full data",lower.panel=panel.shade, upper.panel=panel.conf)
```
```{r}
str(ppp_full)
```
#test model
dataset too large
```{r}
#model1=glm(removed~amount*sba_guaranty_percentage+factor(city)+factor(business_type)+jobs_retained+date_approved+factor(lender)+factor(congressional_district)+factor(processing_method)+factor(loan_status)+term+factor(servicing_lender_name)+factor(rural_urban_indicator)+factor(hubzone_indicator)+factor(business_age_description)+factor(lmi_indicator),data=ppp_test,family = binomial())
#summary(model1)
#plot(model1)
```


#Model1
```{r}
#no date, drop geo info,naics code, not useful char
Xy0=cbind(ppp_full$removed,subset(ppp_full,select=-c(removed)))
str(Xy0)
Xy=subset(Xy0,select=-c(name,state,address,loan_number,sba_office_code,servicing_lender_address,date_approved,forgiveness_date,loan_status_date,city,sba_guaranty_percentage,initial_approval_amount,current_approval_amount,undisbursed_amount,servicing_lender_city,servicing_lender_state,project_city,project_county_name,project_state,originating_lender_city,originating_lender_state,originating_lender_location_id,forgiveness_amount,zip,servicing_lender_zip,project_zip,servicing_lender_location_id,naics_code,lender,servicing_lender_name,loan_status))
Xy[sapply(Xy, is.character)] <- lapply(Xy[sapply(Xy, is.character)], as.factor)
summary(Xy)
#ppp_full[sapply(ppp_full, function(x) ppp_full$removed=='removed'),]
#Xy[sapply(Xy, is.Date)] <- lapply(Xy[sapply(Xy, is.Date)], as.numeric)
#bestglm(Xy, family = binomial(), IC = "AIC")
```
#drop special type of business type,congressional_district to ensure test running
```{r}
summary(Xy$business_type)
#Xy.BT=Xy[!(Xy$business_type=="501(c)19 â€“ Non Profit Veterans" | Xy$business_type=="Rollover as Business Start-Ups (ROB" |Xy$business_type=="Housing Co-op" ),]
Xy.BT=Xy[Xy$business_type %in% names(table(Xy$business_type))[table(Xy$business_type)>10],]

summary(Xy.BT$business_type)
summary(Xy.BT$congressional_district)
tab=table(Xy.BT$congressional_district)
Xy.BT.CD=Xy.BT[Xy.BT$congressional_district %in% names(tab)[tab>2],]
summary(Xy.BT.CD$congressional_district)
summary(Xy.BT.CD$business_type)
```

#train logistic regression model1 with full dataset
```{r}
Xy.BT.CD<-(na.omit(Xy.BT.CD))
summary(Xy.BT.CD)
set.seed(12345)
inTrain <- sample(nrow(Xy.BT.CD), 0.7*nrow(Xy.BT.CD))
train <- data.frame(Xy.BT.CD[inTrain,])
test <- data.frame(Xy.BT.CD[-inTrain,])
modelxy1=glm(train$ppp_full.removed~.,data=train,family = binomial())
summary(modelxy1)
```
ROC and AUC
```{r}
predicted <- predict(modelxy1, type = "response") 
actual <- train$ppp_full.removed

par(pty='s')
roc_modelxy1 <- plot(roc(actual, predicted), print.auc = TRUE, col = "blue",legacy.axes=TRUE)

predicted.probability.test <- predict(modelxy1, type = "response", newdata = test)
ActualTest <- test$ppp_full.removed

roc_rose <- plot(roc(ActualTest, predicted.probability.test), print.auc = TRUE, 
                 col = "green", print.auc.y = .4, add = TRUE)
```

accuracy and baseline accuracy
```{r}
cutoff <- 0.5
prediction <- ifelse(predicted > cutoff, "removed","not removed")
prediction <- factor(prediction,levels=c("removed","not removed"))
(accuracy <- sum(actual == prediction)/nrow(train))
(1-(nrow(ppp_removed_ga)/nrow(ppp_full)))
## Sensitivity
##
(sensitivity <- sum(prediction== "removed" & actual == "removed")/sum(actual == "removed"))
##
## Specificity
##
(specificity <- sum(predicted< cutoff & actual == "not removed")/sum(actual == "not removed"))
##
```
#down sampling Xy2 is a smaller with about 50000samples and equal amount of y/n removed
```{r}
removedset<-Xy.BT.CD[Xy.BT.CD$`ppp_full$removed`=='removed',]
unremovedset<-Xy.BT.CD[Xy.BT.CD$`ppp_full$removed`=='not removed',]
unremovedsetsmall<-sample_n(unremovedset,nrow(removedset))
Xy2<-rbind(removedset,unremovedsetsmall)
```

```{r}
modelxy2=glm(Xy2$`ppp_full$removed`~.,data=Xy2,family = binomial())
summary(modelxy2)
```
```{r}
predicted <- predict(modelxy2, type = "response") 
actual <- Xy2$`ppp_full$removed`

par(pty='s')
roc_modelxy2 <- plot(roc(actual, predicted), print.auc = TRUE, col = "blue",legacy.axes=TRUE)
## Next, the additional argument "add = TRUE" adds the test ROC to the previous plot
predicted.probability.test <- predict(modelxy1, type = "response", newdata = test)
ActualTest <- test$ppp_full.removed

roc_rose <- plot(roc(ActualTest, predicted.probability.test), print.auc = TRUE, 
                 col = "green", print.auc.y = .4, add = TRUE)
```
```{r}
cutoff <- 0.5
prediction <- ifelse(predicted > cutoff, "removed","not removed")
prediction <- factor(prediction,levels=c("removed","not removed"))
(accuracy <- sum(actual == prediction)/nrow(Xy2))

## Sensitivity
##
(sensitivity <- sum(prediction== "removed" & actual == "removed")/sum(actual == "removed"))
##
## Specificity
##
(specificity <- sum(predicted< cutoff & actual == "not removed")/sum(actual == "not removed"))
##
```


```{r}

reg= regsubsets(Xy2$`ppp_full$removed`~.,data=Xy2,  method = "forward")
# II)
regsum= summary(reg)
regsum

names(regsum)

#b)
regsum$bic

#c) 
data.frame(
  Adj.R2 = which.max(regsum$adjr2),
  CP = which.min(regsum$cp),
  BIC = which.min(regsum$bic)
)

# Metric: Adj.R2;  CP; and  BIC suggested 6, 6, 3 predictor variables.

#d)
plot(reg)
```



#add first 2 digit of naics code to the Xy and bank, call it Xy3
```{r}
Xy3=Xy
Xy3$n2code=substr(Xy0$naics_code, start = 1, stop = 2)
Xy3$servicing_lender_name=Xy0$servicing_lender_name
levels(factor(Xy3$n2code))
Xy3=na.omit(Xy3)

```
```{r}
#set too big
#modelxy3=glm(Xy3$`ppp_full$removed`~.,data=Xy3,family = binomial())
#summary(modelxy3)
```
#down sampling 
```{r}
removedset<-Xy3[Xy3$`ppp_full$removed`=='removed',]
unremovedset<-Xy3[Xy3$`ppp_full$removed`=='not removed',]
unremovedsetsmall<-sample_n(unremovedset,nrow(removedset))
Xy4<-rbind(removedset,unremovedsetsmall)
```
```{r}
modelxy4=glm(Xy4$`ppp_full$removed`~.,data=Xy4,family = binomial())
summary(modelxy4)
```
```{r}
toselect.x <-summary(modelxy4)$coeff[-1,4] < 0.001
# select sig. variables
relevant.x <- names(toselect.x)[toselect.x == TRUE] 
# formula with only sig variables
#sig.formula <- as.formula(paste("y ~",relevant.x))  
#sig.formula
coef<-summary(modelxy4)$coeff[-1,3]
sigcoef<-coef[toselect.x]
sigcoef[order(1,)]
```


```{r}
library(car)
vif(modelxy4)
```


```{r}
predicted <- predict(modelxy4, type = "response") 
actual <- Xy4$`ppp_full$removed`

par(pty='s')
roc_modelxy4 <- plot(roc(actual, predicted), print.auc = TRUE, col = "blue",legacy.axes=TRUE)
## Next, the additional argument "add = TRUE" adds the test ROC to the previous plot
#predicted.probability.test <- predict(modelxy4, type = "response", newdata = test)
#ActualTest <- test$ppp_full.removed

#roc_rose <- plot(roc(ActualTest, predicted.probability.test), print.auc = TRUE, col = "green", print.auc.y = .4, add = TRUE)
```
```{r}
cutoff <- 0.5
prediction <- ifelse(predicted > cutoff, "removed","not removed")
prediction <- factor(prediction,levels=c("removed","not removed"))
(accuracy <- sum(actual == prediction)/nrow(Xy4))

## Sensitivity
##
(sensitivity <- sum(prediction== "removed" & actual == "removed")/sum(actual == "removed"))
##
## Specificity
##
(specificity <- sum(predicted< cutoff & actual == "not removed")/sum(actual == "not removed"))
##
```
```{r}
accuracy <- sum(actual == prediction)/nrow(Xy2)
```

```{r}
reg= regsubsets(Xy4$`ppp_full$removed`~.,data=Xy4,  method = "backward",nvmax=70)
# II)
regsum= summary(reg)
regsum

names(regsum)

#b)
regsum$bic

#c) 
data.frame(
  Adj.R2 = which.max(regsum$adjr2),
  CP = which.min(regsum$cp),
  BIC = which.min(regsum$bic)
)

# Metric: Adj.R2;  CP; and  BIC suggested 6, 6, 3 predictor variables.

#d)
plot(reg)
```
```{r}
par(mfrow=c(2,2))
plot(regsum$rsq,xlab="Number of Variables",ylab="RSquare",type="l")
# 
# Adjusted R-Square
plot(regsum$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
(which.max(regsum$adjr2))

#
# The easiest way to find the coefficients when you know the size of the optimal model:
coef(reg,11)
#
# The other criteria
# The following is - in the context of linear regression - same as AIC
plot(regsum$cp,xlab="Number of Variables",ylab="Cp",type='l')
which.min(regsum$cp)

which.min(regsum$bic)
plot(regsum$bic,xlab="Number of Variables",ylab="BIC",type='l')

par(mfrow=c(1,1))
```

```{r}
modelxy6<-glm(Xy4$`ppp_full$removed`~business_type+processing_method+term+hubzone_indicator+business_age_description+lmi_indicator,data=Xy4,family = binomial())
summary(modelxy6)
```
```{r}
predicted <- predict(modelxy6, type = "response") 
actual <- Xy4$`ppp_full$removed`

par(pty='s')
roc_modelxy4 <- plot(roc(actual, predicted), print.auc = TRUE, col = "blue",legacy.axes=TRUE)

cutoff <- 0.5
prediction <- ifelse(predicted > cutoff, "removed","not removed")
prediction <- factor(prediction,levels=c("removed","not removed"))
(accuracy <- sum(actual == prediction)/nrow(Xy2))

## Sensitivity
##
(sensitivity <- sum(prediction== "removed" & actual == "removed")/sum(actual == "removed"))
##
## Specificity
##
(specificity <- sum(predicted< cutoff & actual == "not removed")/sum(actual == "not removed"))
```



#Try to predict the amount received by non removed set by using linear model
```{r}
Xy4nonr<-Xy4[Xy4$`ppp_full$removed`=='removed',]
Xy5=Xy4[,2:12]
modelxy5=lm(amount~.,data=Xy5)
summary(modelxy5)
```


```{r}
dXy4=dummy_cols(Xy4)
summary(dummyXy4)
corrset=data.frame(dXy4$`ppp_full$removed`,dXy4$business_type_Cooperative,dXy4$`business_type_501(c)6 â€“ Non Profit Membership`,dXy4$`business_type_Subchapter S Corporation`,dXy4$jobs_retained,dXy4$term,dXy4$processing_method_PPS,dXy4$hubzone_indicator_Y,dXy4$lmi_indicator_Y,dXy4$n2code_21,dXy4$n2code_23,dXy4$n2code_31,dXy4$n2code_32,dXy4$n2code_33,dXy4$n2code_42,dXy4$n2code_44,dXy4$n2code_45,dXy4$n2code_48,dXy4$n2code_49,dXy4$n2code_51,dXy4$n2code_52,dXy4$n2code_53,dXy4$n2code_54,dXy4$n2code_56,dXy4$n2code_61,dXy4$n2code_62,dXy4$n2code_71,dXy4$n2code_72,dXy4$n2code_81,dXy4$n2code_92,dXy4$n2code_99)
corrgram(corrset)
#cor(corrset)
#corrgram(dummy_cols(Xy4nonr))
```

#knn
#take 20000 samples from a equal proportion dataset


```{r}


set.seed(123)
Xy4min=sample_n(Xy4,size=10000)
Xy.knn.dummy=dummy_cols(Xy4min)

fun <- function(x){ 
  a <- mean(x) 
  b <- sd(x) 
  (x - a)/(b)
}
Xy.knn.dummy=na.omit(Xy.knn.dummy[, sapply(Xy.knn.dummy, is.numeric)])
Xy.knn.dummy=Xy.knn.dummy[,-4]
Xy.knn.dummy=Xy.knn.dummy[,!sapply(Xy.knn.dummy, function(x) mean(x)==0)]
Xy.knn.dummy.norm <- data.frame(apply(Xy.knn.dummy[,-4], 2, fun))
Xy.knn.dummy.norm$removed=Xy.knn.dummy$`ppp_full$removed_removed`
intrain = sample(seq_len(nrow(Xy.knn.dummy.norm)),size = nrow(Xy.knn.dummy.norm)*0.6)
dftrain =Xy.knn.dummy.norm[intrain,]
dftemp =Xy.knn.dummy.norm[-intrain,]
inval = sample(seq_len(nrow(dftemp)),size = nrow(dftemp)*0.6)
dfvalidation=dftemp[inval,]
dftest=dftemp[-inval,]
len=length(Xy.knn.dummy.norm)
```

```{r}
# knn() may be found in the library class
library(class)
# 
train_input <- as.matrix(dftrain[,-len])
train_output <- as.vector(dftrain[,len])
validate_input <- as.matrix(dfvalidation[,-len])
test_input <- as.matrix(dftest[,-len])
#
#
kmax <- 15
ER1 <- rep(0,kmax)
ER2 <- rep(0,kmax)
#
for (i in 1:kmax){
prediction <- knn(train_input, train_input,train_output, k=i)
prediction2 <- knn(train_input, validate_input,train_output, k=i)
prediction3 <- knn(train_input, test_input,train_output, k=i)
#
# The confusion matrix for training data is:
CM1 <- table(prediction, dftrain[,len])
# The training error rate is:
ER1[i] <- (CM1[1,2]+CM1[2,1])/sum(CM1)
# The confusion matrix for validation data is: 
CM2 <- table(prediction2, dfvalidation[,len])
ER2[i] <- (CM2[1,2]+CM2[2,1])/sum(CM2)
}
```


```{r}
plot(c(1,kmax),c(0,0.4),type="n", xlab="k",ylab="Error Rate")
lines(ER1,col="red")
lines(ER2,col="blue")
legend(9, 0.1, c("Training","Validation"),lty=c(1,1), col=c("red","blue"))
z <- which.min(ER2)
cat("Minimum Validation Error k:", z)
#
# Scoring at optimal k
prediction <- knn(train_input, train_input,train_output, k=z)
prediction2 <- knn(train_input, validate_input,train_output, k=z)
prediction3 <- knn(train_input, test_input,train_output, k=z)
#
```


```{r}
CM1 <- table(prediction, dftrain[,len])
CM2 <- table(prediction2, dfvalidation[,len])
CM3 <- table(prediction3, dftest[,len])
CM1
CM2
CM3
ER3 <- (CM3[1,2]+CM3[2,1])/sum(CM3)
1-ER3
## Sensitivity
##
(sensitivity <- CM3[1,1]/(CM3[1,2]+CM3[1,1]))
##
## Specificity
##
(specificity <- CM3[2,2]/(CM3[2,2]+CM3[2,1]))
##
#
```


```{r}
# 
# Now we compute the lift curve for k=15. 
prediction3 <- knn(train_input, test_input, train_output, k=15, prob=T)
#
predicted.probability <- attr(prediction3, "prob")
# 
# This (unfortunately returns the proportion of votes for the winning class - P(Success))
#
predicted.probability <- ifelse(prediction3 ==1, predicted.probability, 1-predicted.probability)
#
df1 <- data.frame(prediction3, predicted.probability,dftest[,len])
# When prediction is 1, we will use predicted.probability; else use 1-predicted.probability
df1S <- df1[order(-predicted.probability),]
df1S$Gains <- cumsum(df1S$dftest...len.)
plot(df1S$Gains,type="n",main="Lift Chart",xlab="Number of Cases",ylab="Cumulative Success")
lines(df1S$Gains,col="blue")
abline(0,sum(df1S$dftest...len.)/nrow(df1S),lty = 2, col="red")
#predknn<-predict(prediction3)
#par(pty='s')
#roc_modelknn <- plot(roc(dftest[,len],predknn ), print.auc = TRUE, col = "blue",legacy.axes=TRUE)

```

```{r}

```

